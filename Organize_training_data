# -*- coding: utf-8 -*-


#
# Organize training dataset
#

import os
import numpy as np
import pandas as pd
import datetime


global dirs, scale
dirs = r'F:\Data\Demand'
scale = ['national','regional','city']


def dynamic_variables_path():
    #
    # Get demand_files and variables files path for each country / region / city 
    # Please change the coding work folder at your own computer
    #
    demand_files = []
    dynamic_variables_files = []
    for i in range(len(scale)):
        file_path = os.path.join(dirs,scale[i])
        loc = os.listdir(file_path)
        for j in range(len(loc)):
            #filter .xlsx file
            files = os.listdir (os.path.join(dirs,scale[i],loc[j]))
            demand_file = []
            dynamic_variables_file = []
            for z in range(len(files)):
                formats = os.path.splitext( os.path.join(dirs,scale[i],loc[j],files[z]) )[-1]
                if formats == '.xlsx' and files[z] != 'dynamic_variables.xlsx':
                    demand_file.append( os.path.join(dirs,scale[i],loc[j],files[z]) )
                elif files[z] == 'dynamic_variables.xlsx':
                    dynamic_variables_file.append( os.path.join(dirs,scale[i],loc[j],files[z]) )
                    
            demand_files.append(demand_file)
            dynamic_variables_files.append(dynamic_variables_file)
    return demand_files, dynamic_variables_files    



def main():
    #
    # read data
    #
    demand_files, dynamic_variables_files = dynamic_variables_path()
    
    for i in range(0,len(demand_files)):       
        #
        # read actual demand data
        #        
        for d in range(len(demand_files[i])):
            demand_data = pd.read_excel(demand_files[i][d],index_col=0)
            if d==0:
                demand = demand_data
            else:
                demand = pd.concat((demand,demand_data))
        demand = demand.reset_index(drop=True)
        p = os.path.split(demand_files[i][d])
        name = os.path.split(p[0])[1]
        demand['name'] = name
        
        if np.unique(demand['year']).min()>2019:
            continue
        
        #
        # read temperature data
        #
        x = pd.read_excel(os.path.join(p[0],'dynamic_variables.xlsx'),sheet_name='temperature_landscan')
        x.rename(columns={'Unnamed: 0': 'year'}, inplace=True)
        if 'temp' in vars():
            del temp
        for y in np.unique(demand['year']):
            if y>2019 or y<2000:
                continue
            t = x.loc[y-2000].drop('year')
            t = pd.DataFrame(t[t != 0])
            t.columns = ['temp']
            t['year'] = y
            t['cum_hour'] = t.index
            if 'temp' not in vars():
                temp = t
            else:
                temp = pd.concat((temp,t))
                
        #
        # read GDP and population data
        #
        x = pd.read_excel(os.path.join(p[0],'dynamic_variables.xlsx'), sheet_name='pop_landscan_and_GDP_Kummu')   
        x.rename(columns={'Unnamed: 0': 'year'}, inplace=True)
        if 'gdp_pop_nlt' in vars():
            del gdp_pop_nlt
        for y in np.unique(demand['year']):
            if y>2019 or y<2000:
                continue
            t = x.loc[y-2000].drop('year')   
            
            if y%4 == 0:
                hours=8784
            else:
                hours=8760
                
            ppp = [t['POP'] for hour in range(hours)]
            gdp = [t['GDP'] for hour in range(hours)] # unit:10**6 USD / PPP
            nlt = [t['NLT'] for hour in range(hours)]
            
            gp = pd.DataFrame(np.array([ppp,gdp,nlt]).T)
            gp.columns=['pop','gdp','nlt']
            gp['year']=y
            gp['cum_hour']=gp.index
            if 'gdp_pop_nlt' not in vars():
                gdp_pop_nlt = gp
            else:
                gdp_pop_nlt = pd.concat((gdp_pop_nlt,gp))  
                
        #
        # read CO2 emission data
        #   
        x = pd.read_excel(os.path.join(p[0],'dynamic_variables.xlsx'), sheet_name='co2_sector')   
        x.rename(columns = {'Unnamed: 0': 'year'}, inplace=True)
        if 'co2_sector' in vars():
            del co2_sector
        for y in np.unique(demand['year']):
            if y>2019 or y<2000:
                continue
            t = x.loc[y-2000].drop('year')   
       
            c = pd.DataFrame(np.array([t]))
            c.columns = t.index
            c['year'] = y

            if 'co2_sector' not in vars():
                co2_sector = c 
            else:
                co2_sector = pd.concat((co2_sector,c))                    
        
        m = pd.merge(demand, temp, on = ['year','cum_hour'])
        m = pd.merge(m, gdp_pop_nlt, on = ['year','cum_hour'])
        m = pd.merge(m, co2_sector, on = ['year'])
        m.to_excel( os.path.join(r'F:\Data\Demand\train\tables',name+'.xlsx') ) # Please change the coding work folder at your own computer
        print(i)
        del m


def loction():
    #
    # get centroid for each country / region / city
    #
    import geopandas as gpd
    lon = []
    lat = []
    name = []
    for scale in ['national','regional','city']:
        folder = os.listdir(os.path.join(r'F:\Data\Demand', scale))
        for i in range(0,len(folder)):       
            path = os.path.join(os.path.join(r'F:\Data\Demand', scale, folder[i]))
            boundary = gpd.read_file( path )
            if len(boundary)>1:
                boundary = boundary.dissolve() 
            x = boundary.centroid[0]
            lon.append(x.coords[0][0])
            lat.append(x.coords[0][1])
            name.append(folder[i])
    location = pd.DataFrame(np.array([name,lon,lat]).T,columns=['name','lon','lat'])
    location.to_csv(r'F:\Data\Demand\location.csv') # Please change the coding work folder at your own computer



def combine_tables():
    #
    # combine tables for all years
    #

    # covert UTC + 0:00 to loctime
    # Please change the coding work folder at your own computer
    timezones = pd.read_excel(r'F:\Data\Demand\Timezone.xlsx') 
            
    path = r'F:\Data\Demand\train\tables'
    files = os.listdir(path)
    for i in range(0,len(files)):
        d = pd.read_excel( os.path.join(path,files[i]) )
        tz = float(timezones[timezones['name'] == files[i][:-5]]['timezone'])
        dt = [datetime.datetime(d['year'][j],d['month'][j],d['day'][j],d['hour'][j]) + datetime.timedelta(hours=tz) for j in range(len(d))]
        d['year'] = [dt[j].year for j in range(len(dt))]
        d['month'] = [dt[j].month for j in range(len(dt))]
        d['day'] = [dt[j].day for j in range(len(dt))]
        d['hour'] = [dt[j].hour for j in range(len(dt))] 
        if i == 0:
            data = d
        else:
            data = pd.concat((data,d))       
        print(i)
        
    # remove columns not used
    data = data.drop(['Unnamed: 0','minitue','time','city','Date'], axis=1) 
    data = data[ (~pd.isna(data['demand_MW'])) & (data['demand_MW']!=0)] 
    data = data.reset_index(drop=True)
    
    # add weekday: 1-7
    weekday = [datetime.date(data['year'][i], data['month'][i], data['day'][i]).isoweekday() for i in range(len(data))]
    data['weekday'] = weekday
    
    # add other indicators 
    data['demand_perCapita'] = data['demand_MW']/data['pop']*1000 # kWh per capita
    data['gdp_perCapita'] = data['gdp']/data['pop']*1000000  # ppp(2011 dollar) per capita
    for c in ['ENE', 'RCO', 'NMM', 'CHE', 'IRO', 'NFE']:
        data[c] = data[c] / data['pop'] * 1e+6 #kg CO2 /pop /km-2

    # combine lon/lat information
    location = pd.read_csv(r'F:\Data\Demand\location.csv', index_col=0)
    data = pd.merge(data,location,on='name',how='left')
    return data



def revised_city_dp(data):
    #
    # Modify the city demand_perCapita by replacing corresponding pop data, see https://github.com/Ecohen4/Energy/tree/master/data
    #
    import re
    dicts = pd.read_excel(r'F:\Data\Energy-master\data\census\XLSX_WUP2014-F12-Cities_Over_300K.xlsx', sheet_name='DATA') # Please change the coding work folder at your own computer
    dicts.columns = dicts.iloc[15]
    dicts = dicts.iloc[16:]
    
    urban_pop = np.full((36,21), np.nan, dtype=np.float32)
    folder = os.listdir(os.path.join(r'F:\Data\Demand\city'))
    for i in range(0,len(folder)):       
        pop = np.full(21,np.nan,dtype=np.float32)
        #country=re.split('_',folder[i])[1]
        city = re.split('_',folder[i])[0]
        if city == 'Beirut':
            city = 'Bayrut (Beirut)'
        if city == 'Los Angeles':
            city = 'Los Angeles-Long Beach-Santa Ana'
        if city == 'City of North Little Rock':
            city = 'Little Rock'
        if city == 'Springfield':
            city = 'Springfield, Missouri'
        z = dicts[(dicts['Urban Agglomeration']==city)][[2000,2005,2010,2015,2020]]
        if len(z) == 1:
            for year in [2000,2005,2010,2015,2020]:
                pop[year-2000] = z[year]
            pop = pd.Series(pop).interpolate()
            urban_pop[i,:] = pop
        else:
            print(i,city)
            
    import warnings
    warnings.filterwarnings('ignore')
    
    data = data[~data['name'].isin(['Eugene_US','IslaSaoNicolau_CaboVerde','Kupang_Indonesia','Tacoma_US','Tema_Ghana','Louisville_US'])] 
    folder = os.listdir(os.path.join(r'F:\Data\Demand\city'))
    for i in range(0,len(folder)):  
        if folder[i] in ['Eugene_US','IslaSaoNicolau_CaboVerde','Kupang_Indonesia','Tacoma_US','Tema_Ghana','Antigua_Guatemala','NewYorkCity_US','MbabaneCity_Swaziland']:  
            continue
        else:
            z = data[data['name']==folder[i]] 
            for year in np.unique(z['year']):
                data['demand_perCapita'][(data['year']==year)&(data['name']==folder[i])]=data['demand_MW'][(data['year']==year)&(data['name']==folder[i])] / urban_pop[i,year-2000]
            print(folder[i])
    return data



def revised_city_gp(data):
    #
    # Modify the city gdp_perCapita by replacing corresponding pop data
    #
    folder = os.listdir(os.path.join(r'F:\Data\Demand\city'))
    city_gdp = pd.read_csv(r'F:\Data\Demand\city_gdp_perCapita.csv',index_col=0)
    for i in range(0,len(city_gdp)):  
        if city_gdp.index[i] in ['Eugene_US','IslaSaoNicolau_CaboVerde','Kupang_Indonesia','Tacoma_US','Tema_Ghana','Antigua_Guatemala','NewYorkCity_US','MbabaneCity_Swaziland','Louisville_US']:  
            continue
        else:
            z=data[data['name']==city_gdp.index[i]] 
            for year in np.unique(z['year']):
                data['gdp_perCapita'][(data['year']==year)&(data['name']==city_gdp.index[i])]=city_gdp[str(year)][i]
            print(folder[i])
    return data



def revised_nation_region_gp(data):
    #
    # calculate the national and regional gdp_perCapita
    #
    nation_region_gdp = pd.read_csv(r'F:\Data\Demand\nation_region_gdp_perCapita.csv',index_col=0) # Please change the coding work folder at your own computer
    for i in range(0,len(nation_region_gdp)):  
        z = data[data['name']==nation_region_gdp.index[i]] 
        for year in np.unique(z['year']):
            if year == 2020:
                year_i = 2019
            elif year == 1999:
                year_i = 2000
            else:
                year_i = year
            data['gdp_perCapita'][(data['year']==year)&(data['name']==nation_region_gdp.index[i])]=nation_region_gdp[str(year_i)][i]
        print(i)
    return data


#
# main
#
main()
data = combine_tables()
data = revised_city_dp(data)
data = revised_city_gp(data)
data = revised_nation_region_gp(data)

#
# save data
#
data.to_csv(r'F:\Data\Demand\train\train_data.csv')


